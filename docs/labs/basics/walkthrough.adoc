:walkthrough: Lab Introduction
:user-password: openshift
:namespace: {user-username}

:experimental:

:article-url: https://developers.redhat.com/articles/2023/08/15/some-article-url

// :btn-text: my text
// :btn: pass:attributes[<code><mark style="background-color: dodgerblue; color: white">&nbsp;{btn-text}&nbsp;</mark>]

ifdef::env-github[]
endif::[]

[id='lab-intro']
= AI basics - Lab1

Explore, build, test and deploy a Camel X demo application using the Developer Sandbox and OpenShift Dev Spaces.

Start your AI journey by learning the very basics of creating, deploying, and invoking a model.

This hands-on lab is based on the following blog article in Red Hat Developers:

* link:{article-url}[window="_blank", , id="rhd-source-article"]

{empty} +

Assuming you have followed the article's instructions, you should be all set to get hands-on with the basics of AI to build applications in the _OpenShift Dev Spaces_ workspace.

For illustration purposes, the picture below shows what the integration end-to-end flow looks like.

// image::images/00-demo-end2end.png[align="center", width=80%]

A client invokes an OpenApi service. A _Camel_ route attends the call, translates the JSON input into XML and calls a backend service to obtain an XML response, then it's translated back to JSON before responding to the original service call.

{empty} +


[time=1]
[id="section-one"]
== Create an OpenShift AI workbench

Let's first start by opening Red Hat OpenShift AI in the Developer Sandbox.

To open the _OpenShift_ console, from _VS Code_ follow the actions below, as illustrated:

. Copy the command below:
+
[source,bash,subs=]
----
> Dev Spaces: Open OpenShift Console
----
+
{blank}
. From VS Code, click the search bar at the top of the window.
. Paste the command:
- Linux: kbd:[Ctrl+v] and press kbd:[Enter] 
- Mac: kbd:[‚åò+v] and press kbd:[Enter]
+
{blank}
. When prompted, click the button `pass:[<mark style="background-color: dodgerblue; color: white">Open</mark>]` 
+
image::images/01-open-openshift-console.png[width=60%]
+
{blank}
. Make sure the console is in _Developer_ view, and you see the _Topology_.
+
image::images/13-openshift-console-developer.png[width=20%]

{empty} +

From the OpenShift console:

. click the apps icon at the top of the screen
. select Red Hat OpenShift AI
+
image::images/02-open-openshift-ai.png[width=40%]
+
{blank}
// reset counter

. From OpenShift AI, select on the left menu _Data Science Projects_
// . In your project's row, click on the link "[blue]#Create a workbench#"
. In your project's row, click on the link pass:[<font style="color: blue">"Create a workbench"</font>]

+
image::images/03-ai-create-workbench.png[width=90%]

{empty} +

Now, enter the following field values to create your AI workbench.

- Name: `wb`
- Notebook image -> Image selection: `TensorFlow`
- Deployment size -> Container size: `Medium`
+
[NOTE]
Leave all other fields with their default values

Then, click at the bottom the `pass:[<mark style="background-color: dodgerblue; color: white">Create workbench</mark>]`

{empty} +

---

{blank}

Your workbench will start provisioning.
It's status will transition from:

- Starting... -> ‚úì Running
+
{blank}

// - pass:[<font style="color: blue">Starting...</font>] -> pass:[<font style="color: blue">‚úì Running</font>]
// +
// {blank}

Once in `‚úì Running` state,


. Click pass:[<font style="color: blue">Open</font>] to launch the workbench in _JupyterLab_.
+
image::images/04-ai-open-workbench.png[width=90%]
+
{empty} +

. Log in with...
+
[square]
* pass:[<font style="border-width:1px; border-style:solid; border-color:blue; color: blue">&nbsp; DevSandbox &nbsp;</font>]
+
{empty} +

. Accept the **Authorize Access** options:
+
[frame=none,grid=none]
|===
| ‚òë user:info
| ‚òë user:check-access
|===
+
// --
// [none]
// * ‚òë user:info
// * ‚òë user:check-access
// --
// +
{blank}
+
And click the `Allow selected permissions` button.

{empty} +

---

{blank}

JupyterLab will open in a new tab.

Clone the source code repository following the actions below:

. Click the _Git Clone_ button
. Copy & paste the repository below:
+
[source,subs=]
----
https://github.com/brunoNetId/redbag-ai
----
+
{blank}
. Click the `pass:[<mark style="background-color: dodgerblue; color: white">&nbsp;Clone&nbsp;</mark>]` button
+
image::images/05-ai-clone-repo.png[width=70%]

{empty} +

After the project is cloned, make sure you change to the following directory in your project tree:

üìÅ `/redbag-ai/workbench`

{empty} +

You should see in your browser a window similar to:

image::images/06-ai-jupyterlab.png[width=80%]

{empty} +

[type=verification]
Do you have JupyterLab open and your source code cloned from GitHub?

[type=verificationSuccess]
You're good to continue with the next step.

[type=verificationFail]
Review the instructions and try to spot where you might have deviated.


{empty} +

[time=3]
[id="section-two"]
== Execute an inference

In this section of the tutorial you'll run, from _JupyterLab_, a _Notebook_ containing code which loads an AI model and runs an inference (prediction) against it.

But first, it's important to get familiar with the base model you will be working with. Continue reading below for a quick overview of the model.

{empty} +

=== Introduction to MobileNet V2

The base model used in the code, _MobileNetV2_, is a well known CNN (_Convolutional Neural Network_), very efficient for image classification tasks.

The model is composed of a series of layers, it takes an image as an input, and produces a probabilistic distribution as an output. +
The inner workings of the model are better understood if summarised in two main stages:

. *Feature extraction*
+
For a given image input, the model extracts features (information) such as color, texture, edges, corners, shapes.

. *Classification*
+
Series of neuronal network layers the information from the extraction traverses and results in a distribution of probabilities.

{empty} +

You can find both stages, feature extractions and classification, in the diagram below, describing in more detail the end to end process.

image::images/07-ai-cnn-diagram.png[width=80%,align=center]

The feature extraction concentrates in synthesising the image into a flat structure of data fed into the classification phase. It's the trained neuronal network that computes the probabilities of matching the input with its dictionary of possible choices.

{empty} +

=== Execute the Notebook

At this point you should have your JupyterLab environment open. Let's run your your first inference.

From the project explorer on the left hand side of your window, double-click on the following resource (notebook):

* redbag-ai -> workbench -> `**redbag-base.ipynb**`

{blank}

Explore the code inside the notebook. +
In summary, the code implements the following logic:

. Loads a pre-trained model.
. Tests a single image (banana).
. Saves the base model to disk.

{empty} +

The code is essentially using a banana image to infer the model and display the prediction result.

To execute all the code at once, use the menu and select:

* *Run* -> *Run All Cells*
+
[NOTE]
--
The upper right corner shows a progress wheel. Wait until finished (idle state):

image::images/11-ai-notebook-wheel.png[width=40%]
--
+
{blank}

Your JupyterLab environment will run all the code and render the output of all executed cells.

The most notable parts of the execution are:

. The input image is converted into a _Tensor_, a multi-dimensional array, before it can be handed over to the model to infer it.

. The inference output is also given as a _Tensor_, and looks similar to:
+
----
1/1 [==============================] - 1s 804ms/step
[[2.15310047e-05 1.42507633e-05 3.33830462e-06 7.00388591e-06
  1.16191713e-05 3.88226545e-05 1.25413470e-04 6.46742546e-06
  1.66206009e-05 3.67777284e-05 2.33631945e-05 1.30055469e-05
  ...            ...            ...            ...
  8.52387075e-06 1.84521323e-05 2.47112821e-05 5.01178838e-05
  9.12087944e-06 1.34241609e-05 7.77729929e-06 7.68292466e-06
  3.32153577e-05 3.97006515e-05 1.97341960e-05 1.06714460e-05]]
----
+
{blank}

. The highest Tensor value is extracted and matched to the `banana` label. You should see a cell with the following output:
+
----
['banana']
----
+
{blank}

. The model is persisted to disk (to be used in the next chapter).
+
NOTE: You'll find in your workbench folder, a new [üìÅ `models`] folder where your base model has been persisted.

{empty} +

=== Chapter conclusions

Executing a prediction was cool, but note in the illustration below the vertical arrays. These arrays are the _Tensor_ data that goes in and out of the model: 

image::images/08-ai-tensor-in-out.png[width=80%, align=center]

{empty} +

For the process above to happen, the developer requires to convert the image into _Tensor_ data, and also to handle _Tensor_ output before analysing the result.

It's rather inconvenient for traditional developers, unfamiliar with AI libraries, to deep dive into the _Tensor_ world in order to build their AI powered applications.

In the next chapter you'll learn a strategy to encapsulate the complexity of handling _Tensors_ so that developers can integrate with an interface easy to work with.

{empty} +
 
[type=verification]
Did you see the execution predict the `banana` label?

[type=verificationSuccess]
You're ready to jump to the next chapter !!

[type=verificationFail]
Inspect in the cell outputs to investigate the possible causes of failure.



[time=2]
[id="section-three"]
== Define a model interface

The previous chapter hinted that it's not easy for developers to use models as they are required to learn and use _Tensors_.

A good strategy is to apply "separation of concerns". Let the developer focus on implementing the application, hide AI complexity, and make the model easy to consume.

The diagram below shows the approach that simplifies the way developers can integrate AI capabilities to their applications.

image::images/09-ai-interface-in-out.png[width=80%, align=center]


Compared to the diagram shown in the previous chapter, the one above shows a much easier interface to work with.

. The model input is *Base64*
. The model output is a *String*
+
{blank}

Developers can easily deal with the types above. Converting an image to Base64 is very easy, and obtaining a plain String as a result is a walk in the park.

{empty} +

=== Create a TensorFlow Signature

In _TensorFlow_, "signatures" are the interface contracts to define input/output specifications.

From the project explorer on the left hand side of your window, double-click on the following resource (notebook):

* redbag-ai -> workbench -> `**redbag-baseSignature.ipynb**`

{blank}

Explore the code inside the notebook. +
In summary, the code implements the following logic:

. Loads the model from disk (saved in the previous chapter)
. Defines the model _Signature_ (and saves the new model)
. Test single image (banana)

{empty} +

The key part is the _Signature_ definition:

image::images/10-ai-signature-code.png[width=40%]

{blank}

The signature is essentially encapsulating what previously the developer was responsible of, that is:

. Converting the image into _Tensor_ information
. Running the prediction
. Processing the result to obtain the label tag

{empty} +

Now, run the notebook by selecting from the menu:

* *Run* -> *Run All Cells*
+
[NOTE]
--
The upper right corner shows a progress wheel. Wait until finished (idle state):

image::images/11-ai-notebook-wheel.png[width=40%]
--
+
{blank}

The notebook includes a `save` operation to persist the new model+signature on disk.

NOTE: You'll find in your models folder a new directory called `redbag/1` (model's name/version).

When done, inspect the last output cell of your notebook. It should show the following result:

[subs="verbatim,quotes"]
----
{'output_0': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'*banana*', b'*0.975945*'], dtype=object)>}
----

[NOTE]
Signatures can also include post-processing logic. Our signature covert the output tensor, from 1000 probability values, to just the highest one and its matching label (banana, 0.975945).


{empty} +


[type=verification]
Did your execution predict a `banana`?

[type=verificationSuccess]
Well done, you're ready to deploy this model !!

[type=verificationFail]
Inspect in the cell outputs to investigate the possible causes of failure.


[time=3]
[id="section-four"]
== Deploy v1 in a Model Server
// == Deploy v1 to a Model Server

The previous task showed how to hide tensor complexity and expose it as an easy to consume interface.

Next, you will complete the encapsulation by deploying the model in a Model Server. +
Below are listed some of the benefits of running Model servers:

- They can run multiple models and manage versions and hot-deployments.
- They can scale as needed to respond to traffic demand.
- Separation of concerns is preserved by keeping applications away from the inferencing engine.

{empty} +

The next set of actions will help you to:

. Setup S3 storage
. Push the model to S3.
. Deploy a Model Server.

{empty} +

=== Setup S3 storage

You will operate from VS Code in _DevSpaces_ to deploy what's needed.

Switch back to your _DevSpaces_ tab in your browser.

image::images/21-ai-at-devspaces.png[width=20%]

{blank}

You can inspect the YAML source you will deploy if you open in your editor the following file:

* deploy -> `**minio.yaml**` 
+
{blank}

The definition contains everything needed to deploy and access the Minio (S3) service.

. Open a terminal from DevSpaces:
+
image::images/12-ai-devspaces-open-terminal.png[width=40%]
+
{blank}

. Execute the following command:
+
[source,console]
----
oc apply -f deploy/minio.yaml 
----
+
{blank}
+
You should see the following output:
+
----
persistentvolumeclaim/minio-pvc created
secret/minio-secret created
deployment.apps/minio created
service/minio-service created
route.route.openshift.io/minio-ui created
----

{empty} +

Now, switch to your console's _Topology_ view tab in your browser.

image::images/22-ai-at-topology-view.png[width=20%]

{blank}

[NOTE]
--
If you don't have the _OpenShift_ console open in a browser tab, click the search bar on top and use the command and actions from the picture below:

image::images/01-open-openshift-console.png[width=60%]
--

In your OpenShift console, find _Minio_'s deployment.

You need to create an S3 bucket. +
Use Minio's UI to do so, follow the instructions below.

. Click on the deployment's link (blue circle)
. Enter in _Minio_ the credentials `**minio**`/`**minio123**`.
. Click on the `pass:[<mark style="background-color: navy; color: white">&nbsp;Login&nbsp;</mark>]` button
. Click on the link pass:[<font style="color: blue">Create a Bucket</font>]
. Enter `**production**` as the bucket name
. Click the button `pass:[<mark style="background-color: navy; color: white">&nbsp;Create Bucket&nbsp;</mark>]`
+
{blank}

image::images/14-ai-minio-open-ui.png[]

{empty} +

Now that your S3 bucket is ready, you can push your model into the bucket.

{empty} +

=== Push model to S3

Switch to your _JupyterLab_ environment.

image::images/23-ai-at-jupyterlab.png[width=20%]

{blank}

Find and open the following Notebook definition:

* redbag-ai -> workbench -> `**redbag-push-latest.ipynb**`

{blank}

The notebook contains code to read the model from your workbench (saved in earlier tasks), and pushes it to the S3 bucket you just created.


Execute the code by selecting from the menu:

* *Run* -> *Run All Cells*
+
[NOTE]
--
The upper right corner shows a progress wheel. Wait until finished (idle state):

image::images/11-ai-notebook-wheel.png[width=40%]
--
+
{blank}

The last cell output should show the following logs:

[subs="verbatim,quotes"]
----
models/*redbag*/*1*/fingerprint.pb
models/*redbag*/*1*/saved_model.pb
models/*redbag*/*1*/variables/variables.index
models/*redbag*/*1*/variables/variables.data-00000-of-00001
---- 

{blank}

The logs show:

- the model is composed of 4 artifacts
- the model's name is *redbag*
- The version uploaded is *1*.

{empty} +

=== Deploy the Model Server

The next step is to deploy the model server.

[NOTE]
We choose to use a _TensorFlow Model Server_ because its interface allows sending Base64 images in JSON fields.

Switch to your _DevSpaces_ tab in your browser.

image::images/21-ai-at-devspaces.png[width=20%]

{blank}

Inspect the following YAML source in your VS Code editor:

* deploy -> `**tensorflow.yaml**` 
+
{blank}

The definition contains everything needed to deploy the _TensorFlow Model Server_.

. From your terminal, execute the following command:
+
[source,console]
----
oc apply -f deploy/tensorflow.yaml 
----
+
{blank}
+
You should see the following output:
+
----
deployment.apps/tf-server created
service/tf-server created
route.route.openshift.io/tf-server created
----

{empty} +

The model server is configured to connect to _Minio_ and will try to read the bucket `production` to find models to serve. It will find our model `redbag v1` and will load it in memory and get ready to serve inference requests.

Switch to your _Topology_ view tab in your browser.

image::images/22-ai-at-topology-view.png[width=20%]

{blank}

Follow the steps below to visually validate your server started successfully:

. Click on the `tf-server` deployment
. Click on the view logs link.
. Inspect the logs where you should find the following trace:
+
----
Successfully loaded servable version {name: redbag version: 1}
----

{blank}

image::images/15-ai-tf-deployed.png[width=100%]

{empty} +

[type=verification]
Did you see a trace in the server logs showing the model successfully loaded?

[type=verificationSuccess]
Well done, you're ready to test the model !!

[type=verificationFail]
Double check you followed the instructions as documented and try again.
 

[time=3]
[id="section-test-model-server"]
== Use the API to send an inference request

When the server starts and reads the model from S3, it automatically exposes a JSON interface that maps inputs and outputs to/from the signatures defined in the model.

We can interact using the JSON interface, in a client/server manner, as applications would.

The diagram below describes our targeted test.

image::images/16-ai-curl-test-banana.png[width=80%,align=center]

{blank}

In the picture above, `curl` loads the same picture we've been using all along, and produces a JSON request with the Base64 encoded image. The server handles the request, executes the inference and returns the result, in JSON format.

Let's run the test. +
Switch to your _JupyterLab_ environment.

image::images/23-ai-at-jupyterlab.png[width=20%]

{blank}

Inspect the following resource:

* redbag-ai -> workbench -> `**infer.sh**`

{blank}

You'll find in the shell script the logic described in the diagram above.

NOTE: The script includes a pipe to `jq` to beautify the JSON result.

To execute the script, open a terminal as follows:

. Select the `Launcher` tab in your _JupyterLab_ window
+
[NOTE]
If the lost your launcher tab, you can open a new one from the menu, select _File -> New Launcher_
+
{blank}

. Click on the terminal icon

image::images/17-ai-jupyterlab-terminal.png[width=60%]

{empty} +

Copy/paste and execute the following command on your terminal:

[source,console]
--
./infer.sh
--

{empty} +

You should obtain the following output:

----
{
  "predictions": [
    "banana",
    "0.975945"
  ]
}
----

{empty} +

[type=verification]
Did you obtain the same JSON message as shown above?

[type=verificationSuccess]
You've successfully run an inference against the Model Server !!

[type=verificationFail]
Review the lab instructions and try again.


[time=4]
[id="section-five"]
== Retrain the model with a custom data set

All the work you've done up until now is rooted on a base model trained with 1000 objects, among those, the famous banana.

However, if only it was possible to re-train the model with our own set of objects, then we could find a good fit for a service our organisation would like to offer. Well, the good news is that it is possible.

The technique to customise a model with your own set of training data is called _Transfer Learning_. Continue reading to learn more.

{empty} +

=== Overview of Transfer Learning

The base pre-trained model (MobileNetV2) you have used in the previous exercises can be illustrated as follows:

image::images/18-ai-transfer-learning-pre-trained.png[width=80%,align=center]

{blank}

The model was trained with a very large data set, and is composed, to put it simply, of a set of convolution layers performing feature extraction, and a neuronal network doing the classification task.

_Transfer Learning_ consists in retaining (freeze) most of the original layers, trained with millions of images, and only re-train new layers, attached to the classifier, replacing the discarded layers.

The image below shows the result of applying _Transfer Learning_:

image::images/19-ai-transfer-learning-re-trained.png[width=80%,align=center]

{blank}

Note in the picture above how most of the original model is kept as-is, only to be stripped from the last layers of the classifier, and replaced with new layers, trained with a new custom data set.

{empty} +

=== Retrain the model applying Transfer Learning

Your _JupyterLab_ project already contains a small data set you can use to retrain the model. The aim is to train a model capable of identifying one type of tea, green tea.

The training set contains 2 classes:

* *Green Tea*: a collection of tea bags of green tea
* *Other*: random pictures of other types of tea.
+
NOTE: Remember the model returns a distribution of probabilities of all trained classes. Training a single class would always result in identifying the same class. The class `Other` allows the model to indicate an input image may not be _Green Tea_.

If you feel curious you can find the training data under:

üìÅ `/redbag-ai/dataset`

{empty} +

Let's get the ball rolling. +

Find and open the following Notebook definition:

* redbag-ai -> workbench -> `**redbag-custom.ipynb**`

{blank}

The notebook contains similar code compared to previous exercises, but includes the critical blocks for Transfer Learning, such as:

* Load and prepare training data
* Import and freeze the base model
* Define and compile the new layers
* Train the model
* Retrain with _Data Augmentation_ (synthetic data)

You'll also find typical data science blocks to render sample data and plot loss/accuracy graphs, to name a couple.

Execute the code by selecting from the menu:

* *Run* -> *Run All Cells*
+
[NOTE]
--
Be patient, the training process should take between 3-5 minutes.

The upper right corner shows a progress wheel. Wait until finished (idle state):

image::images/11-ai-notebook-wheel.png[width=40%]
--
+
{blank}

The notebook includes a `save` operation to persist the new customised model on disk.

NOTE: You'll find in your `models/redbag` folder a new directory `2` indicating the model has evolved from version 1 to version 2.

The last executed cell puts the new model to the test with a test image, using green tea. You should find the following output:

[subs="verbatim,quotes"]
----
{'output_0': <tf.Tensor: shape=(2,), dtype=string, numpy=array([b'*tea-green*', b'*0.592239*'], dtype=object)>}
---- 

{empty} +


[type=verification]
Did you obtain `tea-green` as the predicted result?

[type=verificationSuccess]
You've successfully created a custom model, now ready to be pushed to 'production' !!

[type=verificationFail]
Review the lab instructions and try again.


[time=4]
[id="section-v2-production"]
== Deploy v2 to the Model Server

Now that version 2 is ready, push it to the `production` S3 bucket.

Switch back to the following Notebook:

* redbag-ai -> workbench -> `**redbag-push-latest.ipynb**`

{blank}

And execute it by selecting from the menu:

* *Run* -> *Run All Cells*
+
[NOTE]
--
The upper right corner shows a progress wheel. Wait until finished (idle state):

image::images/11-ai-notebook-wheel.png[width=40%]
--
+
{blank}

The last cell output should show the following logs indicating version 2 has been uploaded:

[subs="verbatim,quotes"]
----
models/*redbag*/*2*/fingerprint.pb
models/*redbag*/*2*/saved_model.pb
models/*redbag*/*2*/variables/variables.index
models/*redbag*/*2*/variables/variables.data-00000-of-00001
---- 

{empty} +

=== Test v2 with an inference request

As we did with our previous banana test, except this time using a sample image of a green tea bag, we'd like to send an inference request via `curl`.

The diagram below describes the test.

image::images/20-ai-curl-test-tea.png[width=80%,align=center]

{blank}

Let's run the test from JupyterLab's terminal and reuse the shell script.

Copy/paste and execute the following command:

[source,console]
--
./infer.sh
--

{blank}

You will probably be disappointed to see the inference result is not predicting _Green Tea_, but instead:

----
{
  "predictions": [
    "other",
    "0.862245"
  ]
}
----

{blank}

Of course! +
The script needs to be updated to read, not the banana image, but the green tea image.

Edit the script ensuring the image loaded is `bali-tea`, as shown below:

----
# image=./samples/banana.jpeg
image=./samples/bali-tea.jpeg
----

{blank}

Then try again. You should obtain this time the following result:

----
{
  "predictions": [
    "tea-green",
    "0.592239"
  ]
}
----

{blank}

{empty} +

[type=verification]
Did your test predicted `tea-green`?

[type=verificationSuccess]
Congratulations, you've created, deployed and test a custom AI model !!

[type=verificationFail]
Review the steps in this exercise to identify the cause for failure, and try again.

{empty} +



[time=1]
[id="section-six"]
== Create an AI-enabled Application

We've covered so far the basics of creating, deploying and consuming AI/ML models, tailored (customised) for the needs of your organisation.

However, organisations rarely expose raw AI results to external consumers. They generally need AI as building blocks to create AI-enabled services.

To follow on that need, we will build and deploy a basic application to demonstrate how to expose an API that utilises AI behind the scenes.

NOTE: This task does not intent to show you how an application needs to be built, but rather show an example of putting in place the last piece to expose a service to the outside world.

The use case is simple, to provide a price tag for a product. The client sends an image of a product, and the API resolves it by responding with its price tag.

The illustration below describes the process:

image::images/24-ai-app-flow.png[width=80%,align=center]

{blank}

In the picture above a smart app (in a phone/tablet/browser) consumes the _Price API_. The server application is responsible to run the inference, and based on the result, find the matching price tag from its in-memory product catalogue.

NOTE: We use _Apache Camel_ to implement the _Price API_. _Apache Camel_ provides the means to create the application with minimal effort and easy readability for learners.


Smart apps consume the 

kamel run -t knative-service.enabled=false --resource price-catalogue.json price.yaml


[time=1]
[id="section-six"]
== Clean up your namespace

When you're done playing in the _Developer Sandbox_, you can clean up your Sandbox namespace by un-deploying your Camel `simple` service and stub `end1` using the following _Maven_ `oc:undeploy` command for both:

[source, subs=]
----
mvn oc:undeploy -Popenshift -s configuration/settings.xml<br>
----

{blank}

Executing the command above for both services should leave your topology view clean from routes, services, and other Kubernetes artifacts in your namespace.

{empty} +

[type=verification]
Is your namespace clean from artifacts?

[type=verificationSuccess]
You've successfully cleaned up your namespace !!

[type=verificationFail]
Inspect in the stub logs to investigate possible causes of failure.

{empty} +
